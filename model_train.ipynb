{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80b7962-b166-4cab-a6f6-ba70975ec8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "        get_linear_schedule_with_warmup,\n",
    "        BertTokenizer,\n",
    "        AdamW,\n",
    "        AutoModelForSequenceClassification,\n",
    "        BertForSequenceClassification,\n",
    "        BertModel,\n",
    "        AutoConfig\n",
    "        )\n",
    "from torch.utils.data import DataLoader,dataset\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13f1d9b-57f2-4f71-b14b-8ccde6b87349",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data'\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding=\"UTF-8\") as file:\n",
    "        docus = file.readlines()\n",
    "        newDocus = []\n",
    "        for data in docus:\n",
    "            newDocus.append(data)\n",
    "    return newDocus\n",
    "#建立数据集 \n",
    "class Label_Dataset(dataset.Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "    def __len__(self):#返回数据长度\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,ind):\n",
    "        onetext = self.data[ind]\n",
    "        label, content = onetext.split('\\t',1)\n",
    "        label = torch.LongTensor([int(float(label))])\n",
    "        return content,label\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8f4124-074c-4f39-aa36-5f4f4236188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainContent = read_file(os.path.join(data_dir, \"train1.csv\")) \n",
    "testContent = read_file(os.path.join(data_dir, \"test1.csv\"))\n",
    "valContent = read_file(os.path.join(data_dir, \"val1.csv\"))\n",
    "\n",
    "traindataset =Label_Dataset( trainContent )\n",
    "testdataset =Label_Dataset( testContent )\n",
    "valdataset =Label_Dataset( valContent )\n",
    "\n",
    "testdataloder = DataLoader(testdataset, batch_size=5, shuffle = False)\n",
    "valdataloder = DataLoader(valdataset, batch_size=5, shuffle = False)\n",
    "batch_size = 5\n",
    "traindataloder = DataLoader(traindataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "class_list = [x.strip() for x in open(\n",
    "        os.path.join(data_dir, \"class.txt\"),encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d14ecf9-bd61-4bd5-8117-b513311b5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert/bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = 'bert/bert-base-chinese'#建立模型\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights,padding=True,truncation=True,max_length=512)\n",
    "config = AutoConfig.from_pretrained(pretrained_weights ,num_labels=2) \n",
    "#单独指定config，在config中指定分类个数\n",
    "nlp_classif = AutoModelForSequenceClassification.from_pretrained(pretrained_weights ,config=config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "nlp_classif = nlp_classif.to(device)\n",
    "for p in nlp_classif.parameters():\n",
    "        p.requires_grad=True\n",
    "time_start = time.time() #开始时间\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm =0.1  #梯度剪辑的阀值\n",
    "require_improvement = 1000      # 若超过1000batch效果还没提升，则提前结束训练\n",
    "savedir = './myfinetun-bert_chinese'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4b774f-9a2c-4e58-9d84-1c32814cb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, traindataloder, testdataloder):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                num_warmup_steps=0, num_training_steps=len(traindataloder) * epochs)\n",
    "\n",
    "\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, epochs))\n",
    "        for i, (sku_name, labels) in enumerate(traindataloder):\n",
    "            model.train()\n",
    "            \n",
    "            ids = tokenizer.batch_encode_plus( sku_name,padding=True,truncation=True,max_length=512,\n",
    "                #模型的配置文件中就是512，当有超过这个长度的会报错\n",
    "                return_tensors='pt')#没有return_tensors会返回list！！！！\n",
    "               \n",
    "            labels = labels.squeeze().to(device) \n",
    "            outputs = model(ids[\"input_ids\"].to(device), labels=labels,\n",
    "                            attention_mask =ids[\"attention_mask\"].to(device)  )\n",
    "            \n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            \n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                truelabel = labels.data.cpu()\n",
    "                predic = torch.argmax(logits,axis=1).data.cpu()\n",
    "#                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(truelabel, predic)\n",
    "                print(train_acc)\n",
    "                dev_acc, dev_loss = evaluate( model, testdataloder)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    model.save_pretrained(savedir)                    \n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "def evaluate(model, testdataloder):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for sku_name, labels in testdataloder:\n",
    "            ids = tokenizer.batch_encode_plus( sku_name,padding=True,truncation=True,max_length=512,return_tensors='pt')#没有return_tensors会返回list！！！！\n",
    "               \n",
    "            labels = labels.squeeze().to(device) \n",
    "            outputs = model(ids[\"input_ids\"].to(device), labels=labels,attention_mask =ids[\"attention_mask\"].to(device) )\n",
    "            \n",
    "            loss, logits = outputs[:2]\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.argmax(logits,axis=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    return acc, loss_total / len(testdataloder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1db566-1beb-4015-85f8-22068f5a8f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\transformers\\src\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n",
      "0.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_10828/587383160.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraindataloder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvaldataloder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_10828/3869694358.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, traindataloder, testdataloder)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruelabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mdev_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdataloder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdev_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdev_best_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mdev_best_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_10828/3869694358.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, testdataloder)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mpredic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mlabels_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(nlp_classif, traindataloder, valdataloder)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c97c3fb-9df8-461e-82a7-fd27bf4f8d1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#微调后测试集准确率\n",
    "config = AutoConfig.from_pretrained(savedir,num_labels=2) \n",
    "nlp_classif2 = AutoModelForSequenceClassification.from_pretrained(savedir,config=config).to(device)\n",
    "testContent = read_file(os.path.join(data_dir, \"test1.csv\"))\n",
    "testdataset =Label_Dataset( testContent )\n",
    "testdataloder = DataLoader(testdataset, batch_size=1, shuffle = False)\n",
    "loss_total = 0\n",
    "predict_all = np.array([], dtype=int)\n",
    "labels_all = np.array([], dtype=int)\n",
    "logits_all=[]\n",
    "with torch.no_grad():\n",
    "    for sku_name, labels in testdataloder:\n",
    "        ids = tokenizer.batch_encode_plus( sku_name,truncation=True,\n",
    "            max_length=512,  #模型的配置文件中就是512，当有超过这个长度的会报错\n",
    "            padding=True,return_tensors='pt')#没有return_tensors会返回list！！！！\n",
    "\n",
    "        labels = labels.squeeze().to(device) \n",
    "        outputs = nlp_classif2(ids[\"input_ids\"].to(device), labels=labels,attention_mask =ids[\"attention_mask\"].to(device) )\n",
    "        loss, logits = outputs[:2]\n",
    "        logits_all.append(logits.data.cpu().numpy())\n",
    "        loss_total += loss\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        predic = torch.argmax(logits,axis=1).data.cpu().numpy()\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "        predict_all = np.append(predict_all, predic)\n",
    "acc = metrics.accuracy_score(labels_all,predict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188ba144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd29a850-e4e9-4b91-b9a6-084c3f754908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492142857142857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44695108",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_result.txt\",'w') as f:\n",
    "    for i in predict_all:\n",
    "        f.write(str(i)+' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58aa8a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8c429f-b1b5-496b-91d3-17b4b1f0171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#微调后训练集准确率\n",
    "trainContent = read_file(os.path.join(data_dir, \"train1.csv\"))\n",
    "traindataset =Label_Dataset( trainContent )\n",
    "traindataloder = DataLoader(traindataset, batch_size=1, shuffle = False)\n",
    "loss_total = 0\n",
    "predict_all = np.array([], dtype=int)\n",
    "labels_all = np.array([], dtype=int)\n",
    "logits_all=[]\n",
    "with torch.no_grad():\n",
    "    for sku_name, labels in traindataloder:\n",
    "        ids = tokenizer.batch_encode_plus( sku_name,truncation=True,\n",
    "            max_length=512,  #模型的配置文件中就是512，当有超过这个长度的会报错\n",
    "            padding=True,return_tensors='pt')#没有return_tensors会返回list！！！！\n",
    "        labels = labels.squeeze().to(device) \n",
    "        outputs = nlp_classif2(ids[\"input_ids\"].to(device), labels=labels,attention_mask =ids[\"attention_mask\"].to(device) )\n",
    "        loss, logits = outputs[:2]\n",
    "        logits_all.append(logits.data.cpu().numpy())\n",
    "        loss_total += loss\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        predic = torch.argmax(logits,axis=1).data.cpu().numpy()\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "        predict_all = np.append(predict_all, predic)\n",
    "acc = metrics.accuracy_score(labels_all, predict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3d7a1c-a35e-4f12-8e1f-f81cda9a4a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9530982142857143"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9d883d-3b7a-474b-ae82-4155902214b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型检验\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(\"生意超级好，但服务超级差劲哦！！！ 我们坐下后，没人擦桌子，没人上餐具的一刚！！！！ 后来更离谱来！！！ 虾饺，没醋的， 红豆冰，没勺子的！ 色拉，就一盘菜，没色拉酱的！！！ 什么和什么麻！！！ 怎么会这样的！！！！！ 差到家哦！！！！！！！！\", return_tensors=\"pt\").to(device)\n",
    "    labels = torch.tensor([1]).unsqueeze(0).to(device)  # Batch size 1\n",
    "    outputs = nlp_classif2(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2689ae3f-7a0a-4a2f-a5ae-f277e7c8aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testContent = read_file(os.path.join(data_dir, \"test1.csv\"))\n",
    "testdataset =Label_Dataset( testContent )\n",
    "testdataloder = DataLoader(testdataset, batch_size=1, shuffle = False)\n",
    "nlp_classif.to(device)\n",
    "nlp_classif.eval()\n",
    "loss_total = 0\n",
    "predict_all = np.array([], dtype=int)\n",
    "labels_all = np.array([], dtype=int)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb803f9b-fdb3-4164-81d6-b13695142db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_total = 0\n",
    "predict_all = np.array([], dtype=int)\n",
    "labels_all = np.array([], dtype=int)\n",
    "logits_all=[]\n",
    "with torch.no_grad():\n",
    "    for sku_name, labels in testdataloder:\n",
    "        ids = tokenizer.batch_encode_plus( sku_name,truncation=True,\n",
    "            max_length=512,  #模型的配置文件中就是512，当有超过这个长度的会报错\n",
    "            padding=True,return_tensors='pt')#没有return_tensors会返回list！！！！\n",
    "\n",
    "        labels = labels.squeeze().to(device) \n",
    "        outputs = nlp_classif(ids[\"input_ids\"].to(device), labels=labels,attention_mask =ids[\"attention_mask\"].to(device) )\n",
    "        loss, logits = outputs[:2]\n",
    "        logits_all.append(logits.data.cpu().numpy())\n",
    "        loss_total += loss\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        predic = torch.argmax(logits,axis=1).data.cpu().numpy()\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "        predict_all = np.append(predict_all, predic)\n",
    "acc = metrics.accuracy_score(labels_all, predict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30717209-6f2b-4f24-ba26-05450c2ab9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f22a20af907fde35ff19e1e892fdb271353fb19b11c7ebd774491472e685293c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
